<p align="center">
  <img src="code/checkpoints/fig2_learning_curves.png" width="700" alt="Learning Curves">
</p>

<h1 align="center">ğŸ›¡ï¸ HMARL-SOC</h1>

<p align="center">
  <strong>Hierarchical Multi-Agent Reinforcement Learning for Autonomous Security Operations Center Coordination</strong>
</p>

<p align="center">
  <a href="https://colab.research.google.com/github/nutthakorn7/HMARL-SOC-journal/blob/main/code/train_colab.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"></a>
  <img src="https://img.shields.io/badge/python-3.8%2B-blue" alt="Python">
  <img src="https://img.shields.io/badge/framework-PyTorch-EE4C2C?logo=pytorch" alt="PyTorch">
  <img src="https://img.shields.io/badge/env-Gymnasium-0081A5" alt="Gymnasium">
  <img src="https://img.shields.io/badge/paper-IEEE%20Access-00629B?logo=ieee" alt="IEEE Access">
  <img src="https://img.shields.io/badge/license-MIT-green" alt="License">
</p>

<p align="center">
  <em>Extended version of our <a href="https://github.com/nutthakorn7/HMARL-SOC">ITC-CSCC 2026 paper</a> â€” submitted to IEEE Access</em>
</p>

---

## ğŸ¯ What is HMARL-SOC?

Enterprise SOCs face **thousands of security events per hour**, yet analysts can resolve fewer than half within a shift. HMARL-SOC is a **three-tier hierarchical multi-agent RL architecture** that mirrors the real division of labor in SOC teams:

```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚    ğŸ–ï¸  Strategic Coordinator (PPO)    â”‚  Tier 1
                    â”‚    Campaign decomposition & goals     â”‚
                    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚ d_t       â”‚ d_t       â”‚ d_t
                    â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚ ğŸ” Hunt â”‚  â”‚ ğŸ“Š Triage â”‚  â”‚ ğŸš¨ Response â”‚  Tier 2
                    â”‚  (SAC)  â”‚  â”‚   (DQN)   â”‚  â”‚  (MADDPG)   â”‚
                    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
                         â”‚  Ï„_i        â”‚               â”‚  Ï„_i
                    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
                    â”‚ ğŸ’¾ Shared Replay Buffer + ğŸ” Attention  â”‚  Tier 3
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

Each agent uses the **RL algorithm best suited to its action space**: SAC for continuous threat hunting, DQN for discrete alert classification, and MADDPG for coordinated incident response.

---

## ğŸ“Š Key Results

Performance comparison on a **200-host, 5-segment MITRE ATT&CK simulator** (mean Â± std, 5 seeds):

| Method | Reward (â†‘) | MTTD (â†“) | MTTR (â†“) | FPR % (â†“) | CSR % (â†‘) |
|:-------|:---:|:---:|:---:|:---:|:---:|
| Rule-SOAR | âˆ’1238.2Â±26.6 | **8.0**Â±0.1 | 136.9Â±3.1 | 5.14Â±0.01 | 35.2Â±1.8 |
| Single-DRL | âˆ’336.5Â±34.0 | 10.9Â±1.4 | 95.1Â±12.3 | 2.97Â±0.28 | 65.0Â±7.3 |
| IQL | +1.8Â±4.7 | 22.8Â±18.3 | 91.9Â±56.7 | 0.22Â±0.23 | 67.8Â±29.8 |
| MAPPO | âˆ’292.2Â±12.6 | 8.8Â±0.1 | 78.6Â±0.9 | 2.52Â±0.04 | 69.7Â±0.6 |
| QMIX | âˆ’99.3Â±66.3 | 8.2Â±0.2 | **63.4**Â±32.4 | 1.03Â±0.02 | **77.5**Â±18.6 |
| **HMARL-SOC** | **+6.9**Â±1.0 | 16.8Â±5.3 | 93.2Â±21.3 | **0.17**Â±0.08 | 71.0Â±10.4 |

> **HMARL-SOC achieves the lowest false positive rate (0.17%) â€” a 6Ã— reduction vs QMIX and 15Ã— vs MAPPO â€” and the highest cumulative reward, reflecting the best overall balance across detection, response speed, disruption cost, and false alarm minimization.**

### K-Sensitivity Analysis

<p align="center">
  <img src="code/checkpoints/fig_k_sensitivity.png" width="550" alt="K-Sensitivity">
</p>

The Strategic Coordinator's temporal abstraction factor **K=5** yields optimal performance. At K=1, rapidly changing directives overwhelm operational agents (reward +0.9). At K=10, the SC reacts too slowly (FPR spikes to 11.5%).

---

## ğŸš€ Quick Start

```bash
# Clone
git clone https://github.com/nutthakorn7/HMARL-SOC-journal.git
cd HMARL-SOC-journal/code

# Install
pip install -r requirements.txt

# Train HMARL-SOC (10K episodes)
python train.py --config configs/default.yaml --episodes 10000 --seed 42

# Train all baselines
python train_baselines.py --episodes 10000 --seeds 42 123 456 789 1024

# Evaluate
python evaluate.py --checkpoint checkpoints/checkpoint_best.pt --episodes 2000

# Generate paper figures
python generate_figures.py
```

**Google Colab** (free GPU): click the Colab badge above â˜ï¸

---

## ğŸ“ Project Structure

```
code/
â”œâ”€â”€ configs/
â”‚   â””â”€â”€ default.yaml              # Hyperparameters (Table II in paper)
â”œâ”€â”€ hmarl_soc/
â”‚   â”œâ”€â”€ env/
â”‚   â”‚   â”œâ”€â”€ soc_env.py            # Gymnasium SOC environment (Dec-POMDP)
â”‚   â”‚   â”œâ”€â”€ network.py            # Enterprise network (5 segments Ã— 40 hosts)
â”‚   â”‚   â””â”€â”€ attacker.py           # MITRE ATT&CK 5-phase campaign
â”‚   â”œâ”€â”€ agents/
â”‚   â”‚   â”œâ”€â”€ strategic_coordinator.py  # PPO + GAE (Tier 1, updates every K steps)
â”‚   â”‚   â”œâ”€â”€ threat_hunter.py          # SAC with auto-entropy (Tier 2)
â”‚   â”‚   â”œâ”€â”€ alert_triage.py           # Dueling Double DQN (Tier 2)
â”‚   â”‚   â””â”€â”€ response_orchestrator.py  # MADDPG (Tier 2)
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â””â”€â”€ networks.py           # 3-layer MLP (256 hidden, ReLU)
â”‚   â””â”€â”€ core/
â”‚       â”œâ”€â”€ replay_buffer.py      # Prioritized shared replay buffer (200K)
â”‚       â””â”€â”€ attention.py          # Multi-head attention explainer
â”œâ”€â”€ train.py                      # Main training loop (Algorithm 1)
â”œâ”€â”€ train_baselines.py            # Rule-SOAR, Single-DRL, IQL, QMIX, MAPPO
â”œâ”€â”€ train_qmix.py                 # QMIX with per-segment action targeting
â”œâ”€â”€ evaluate.py                   # Evaluation & metric computation
â”œâ”€â”€ evaluate_cyborg.py            # CybORG CAGE-4 zero-shot transfer
â”œâ”€â”€ generate_figures.py           # Reproduce all paper figures
â”œâ”€â”€ checkpoints/                  # Trained models & training CSVs
â””â”€â”€ requirements.txt
```

---

## âš™ï¸ Hyperparameters

| Category | Parameter | Value |
|----------|-----------|-------|
| **Environment** | Segments / Hosts | 5 / 200 |
| **Training** | Episodes / Seeds | 10,000 / 5 |
| **SC (PPO)** | LR / Clip Îµ / K | 3Ã—10â»â´ / 0.2 / 5 |
| **TH (SAC)** | LR / Î± / Ï„ | 3Ã—10â»â´ / 0.2 / 0.005 |
| **AT (DQN)** | Îµ-greedy decay | 1.0 â†’ 0.05 over 50K |
| **RO (MADDPG)** | LR / Ï„ | 3Ã—10â»â´ / 0.005 |
| **Reward** | Î±, Î², Î´, Î» | 1.0, 1.5, âˆ’0.3, âˆ’2.0 |
| **Complexity** | Per-step cost | O(NÂ·dÂ²) |

---

## ğŸ“„ Citation

```bibtex
@article{chalaemwongwan2026hmarl,
  title   = {{HMARL-SOC}: Hierarchical Multi-Agent Reinforcement Learning
             for Autonomous Security Operations Center Coordination},
  author  = {Chalaemwongwan, Nutthakorn},
  journal = {IEEE Access},
  year    = {2026},
  note    = {Under review}
}
```

Conference version:

```bibtex
@inproceedings{chalaemwongwan2026hmarl_conf,
  title     = {{HMARL-SOC}: Hierarchical Multi-Agent Reinforcement Learning
               for Autonomous {SOC} Operations},
  author    = {Chalaemwongwan, Nutthakorn},
  booktitle = {Proc. ITC-CSCC},
  year      = {2026}
}
```

---

## ğŸ“œ License

This project is licensed under the [MIT License](LICENSE).

---

## ğŸ™ Acknowledgment

During manuscript preparation the author used Claude (Anthropic) for English grammar checking and sentence-level editing. All research design, algorithm development, implementation, experimentation, and interpretation of results were performed solely by the author.

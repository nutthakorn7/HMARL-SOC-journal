<div align="center">

# ğŸ›¡ï¸ HMARL-SOC

### Hierarchical Multi-Agent Reinforcement Learning<br>for Autonomous Security Operations Center Coordination

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/nutthakorn7/HMARL-SOC-journal/blob/main/code/train_colab.ipynb)
[![Python 3.8+](https://img.shields.io/badge/python-3.8%2B-3776AB?logo=python&logoColor=white)](https://python.org)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0%2B-EE4C2C?logo=pytorch&logoColor=white)](https://pytorch.org)
[![Gymnasium](https://img.shields.io/badge/Gymnasium-0.29-0081A5?logo=openaigym&logoColor=white)](https://gymnasium.farama.org)
[![IEEE Access](https://img.shields.io/badge/ğŸ“„_Paper-IEEE_Access-00629B)](https://ieeeaccess.ieee.org)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)

<br>

*Extended version of our [ITC-CSCC 2026 conference paper](https://github.com/nutthakorn7/HMARL-SOC) â€” submitted to IEEE Access*

<br>

<img src="code/checkpoints/fig2_learning_curves.png" width="750" alt="Learning Curves">

<br>

**SOCs face 10,000+ events/hour. Analysts can handle fewer than half.**<br>
**HMARL-SOC automates the entire SOC pipeline â€” achieving 0.17% false positive rate.**

</div>

---

## ğŸ—ï¸ Architecture

The framework mirrors real SOC team structure with **three specialized tiers**, each using the RL algorithm best suited to its action space:

```mermaid
graph TD
    subgraph Tier1["ğŸ–ï¸ Tier 1 â€” Strategic Coordination"]
        SC["Strategic Coordinator<br><b>PPO + GAE</b><br><i>Campaign decomposition</i><br><i>Updates every K=5 steps</i>"]
    end

    subgraph Tier2["âš”ï¸ Tier 2 â€” Operational Agents"]
        TH["ğŸ” Threat Hunter<br><b>SAC</b><br><i>Continuous probing</i>"]
        AT["ğŸ“Š Alert Triage<br><b>Dueling DQN</b><br><i>Discrete classification</i>"]
        RO["ğŸš¨ Response Orchestrator<br><b>MADDPG</b><br><i>Coordinated isolation</i>"]
    end

    subgraph Tier3["ğŸ’¾ Tier 3 â€” Shared Infrastructure"]
        RB["Prioritized Replay Buffer<br><i>200K transitions</i>"]
        ATT["Multi-Head Attention<br><i>Explainability module</i>"]
    end

    SC -->|"d_t: directives"| TH
    SC -->|"d_t: directives"| AT
    SC -->|"d_t: directives"| RO
    TH -->|"Ï„_i: experiences"| RB
    AT -->|"Ï„_i: experiences"| RB
    RO -->|"Ï„_i: experiences"| RB
    RB <-->|"sampling"| ATT

    style Tier1 fill:#1a1a2e,stroke:#e94560,color:#fff
    style Tier2 fill:#16213e,stroke:#0f3460,color:#fff
    style Tier3 fill:#0f3460,stroke:#533483,color:#fff
    style SC fill:#e94560,stroke:#e94560,color:#fff
    style TH fill:#0f3460,stroke:#53a8b6,color:#fff
    style AT fill:#0f3460,stroke:#53a8b6,color:#fff
    style RO fill:#0f3460,stroke:#53a8b6,color:#fff
    style RB fill:#533483,stroke:#533483,color:#fff
    style ATT fill:#533483,stroke:#533483,color:#fff
```

---

## ğŸ“Š Key Results

<div align="center">

Performance on a **200-host, 5-segment MITRE ATT&CK simulator** (mean Â± std, 5 seeds)

| | Reward â†‘ | MTTD â†“ | MTTR â†“ | FPR % â†“ | CSR % â†‘ |
|:---|:---:|:---:|:---:|:---:|:---:|
| Rule-SOAR | âˆ’1238.2Â±26.6 | **8.0**Â±0.1 | 136.9Â±3.1 | 5.14Â±0.01 | 35.2Â±1.8 |
| Single-DRL | âˆ’336.5Â±34.0 | 10.9Â±1.4 | 95.1Â±12.3 | 2.97Â±0.28 | 65.0Â±7.3 |
| IQL | +1.8Â±4.7 | 22.8Â±18.3 | 91.9Â±56.7 | 0.22Â±0.23 | 67.8Â±29.8 |
| MAPPO | âˆ’292.2Â±12.6 | 8.8Â±0.1 | 78.6Â±0.9 | 2.52Â±0.04 | 69.7Â±0.6 |
| QMIX | âˆ’99.3Â±66.3 | 8.2Â±0.2 | **63.4**Â±32.4 | 1.03Â±0.02 | **77.5**Â±18.6 |
| **ğŸ† HMARL-SOC** | **+6.9**Â±1.0 | 16.8Â±5.3 | 93.2Â±21.3 | **0.17**Â±0.08 | 71.0Â±10.4 |

</div>

> [!IMPORTANT]
> **HMARL-SOC achieves the lowest FPR (0.17%)** â€” a **6Ã— reduction** vs QMIX and **15Ã— vs MAPPO** â€” while maintaining the highest cumulative reward, reflecting the best overall balance.

### ğŸ›ï¸ K-Sensitivity Analysis

<div align="center">
<img src="code/checkpoints/fig_k_sensitivity.png" width="550" alt="K-Sensitivity Analysis">

*The Strategic Coordinator's temporal abstraction factor **K=5** is optimal.<br>K=1 overwhelms agents with rapid directive changes. K=10+ reacts too slowly to threats.*
</div>

---

## ğŸš€ Quick Start

```bash
# Clone & install
git clone https://github.com/nutthakorn7/HMARL-SOC-journal.git
cd HMARL-SOC-journal/code
pip install -r requirements.txt

# ğŸ‹ï¸ Train HMARL-SOC
python train.py --config configs/default.yaml --episodes 10000 --seed 42

# ğŸ†š Train all baselines
python train_baselines.py --episodes 10000 --seeds 42 123 456 789 1024

# ğŸ“ˆ Evaluate
python evaluate.py --checkpoint checkpoints/checkpoint_best.pt --episodes 2000

# ğŸ¨ Reproduce paper figures
python generate_figures.py
```

> [!TIP]
> **No GPU?** Use [Google Colab](https://colab.research.google.com/github/nutthakorn7/HMARL-SOC-journal/blob/main/code/train_colab.ipynb) â€” free T4 GPU, pre-configured environment.

---

## ğŸ“ Project Structure

```
code/
â”œâ”€â”€ ğŸ§  hmarl_soc/                    # Core framework
â”‚   â”œâ”€â”€ env/
â”‚   â”‚   â”œâ”€â”€ soc_env.py               # Gymnasium SOC environment (Dec-POMDP)
â”‚   â”‚   â”œâ”€â”€ network.py               # Enterprise network (5 segments Ã— 40 hosts)
â”‚   â”‚   â””â”€â”€ attacker.py              # MITRE ATT&CK 5-phase campaign model
â”‚   â”œâ”€â”€ agents/
â”‚   â”‚   â”œâ”€â”€ strategic_coordinator.py  # PPO + GAE (Tier 1)
â”‚   â”‚   â”œâ”€â”€ threat_hunter.py          # SAC with auto-entropy (Tier 2)
â”‚   â”‚   â”œâ”€â”€ alert_triage.py           # Dueling Double DQN (Tier 2)
â”‚   â”‚   â””â”€â”€ response_orchestrator.py  # MADDPG (Tier 2)
â”‚   â”œâ”€â”€ models/networks.py           # 3-layer MLP (256 hidden, ReLU)
â”‚   â””â”€â”€ core/
â”‚       â”œâ”€â”€ replay_buffer.py          # Prioritized shared replay (200K)
â”‚       â””â”€â”€ attention.py              # Multi-head attention explainer
â”œâ”€â”€ âš¡ train.py                       # Main training loop (Algorithm 1)
â”œâ”€â”€ ğŸ†š train_baselines.py             # All baselines (Rule-SOAR â†’ QMIX)
â”œâ”€â”€ ğŸ“Š evaluate.py                    # Metrics & evaluation
â”œâ”€â”€ ğŸ¨ generate_figures.py            # Reproduce all paper figures
â”œâ”€â”€ ğŸ“¦ checkpoints/                   # Trained models & CSV logs
â””â”€â”€ ğŸ“‹ requirements.txt
```

---

## âš™ï¸ Hyperparameters

<details>
<summary><b>Click to expand full hyperparameter table</b></summary>

| Category | Parameter | Value |
|:---------|:----------|:------|
| **Environment** | Segments / Hosts | 5 / 200 |
| | Attack model | MITRE ATT&CK (5-phase) |
| **Training** | Episodes / Seeds | 10,000 / 5 |
| | Discount Î³ | 0.99 |
| | Batch size | 256 |
| **SC (PPO)** | LR / Clip Îµ / K steps | 3Ã—10â»â´ / 0.2 / 5 |
| **TH (SAC)** | LR / Î± / target Ï„ | 3Ã—10â»â´ / 0.2 / 0.005 |
| **AT (DQN)** | Îµ-decay | 1.0 â†’ 0.05 over 50K steps |
| **RO (MADDPG)** | LR / target Ï„ | 3Ã—10â»â´ / 0.005 |
| **Reward** | Î±, Î², Î´, Î» | 1.0, 1.5, âˆ’0.3, âˆ’2.0 |
| **Complexity** | Per-step cost | O(NÂ·dÂ²) linear in agents |

</details>

---

## ğŸ“„ Citation

```bibtex
@article{chalaemwongwan2026hmarl,
  title   = {{HMARL-SOC}: Hierarchical Multi-Agent Reinforcement Learning
             for Autonomous Security Operations Center Coordination},
  author  = {Chalaemwongwan, Nutthakorn},
  journal = {IEEE Access},
  year    = {2026},
  note    = {Under review}
}
```

<details>
<summary>Conference version (ITC-CSCC 2026)</summary>

```bibtex
@inproceedings{chalaemwongwan2026hmarl_conf,
  title     = {{HMARL-SOC}: Hierarchical Multi-Agent Reinforcement Learning
               for Autonomous {SOC} Operations},
  author    = {Chalaemwongwan, Nutthakorn},
  booktitle = {Proc. ITC-CSCC},
  year      = {2026}
}
```

</details>

---

<div align="center">

**[ğŸ“„ Paper](https://ieeeaccess.ieee.org) Â· [ğŸ› Issues](https://github.com/nutthakorn7/HMARL-SOC-journal/issues) Â· [ğŸ’¬ Discussions](https://github.com/nutthakorn7/HMARL-SOC-journal/discussions)**

MIT License Â© 2026 Nutthakorn Chalaemwongwan

</div>
